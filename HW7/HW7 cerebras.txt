2024-03-31 02:25:06,049 INFO:   Effective batch size is 1024.
2024-03-31 02:25:06,077 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in "model_dir_bert_large_pytorch" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.
2024-03-31 02:25:06,078 INFO:   No checkpoints were found in "model_dir_bert_large_pytorch".
2024-03-31 02:25:06,078 INFO:   No checkpoint was provided. Using randomly initialized model parameters.
2024-03-31 02:25:07,405 INFO:   Saving checkpoint at step 0
2024-03-31 02:25:36,519 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_0.mdl
2024-03-31 02:25:51,854 INFO:   Compiling the model. This may take a few minutes.
2024-03-31 02:25:51,855 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-03-31 02:25:53,205 INFO:   Initiating a new image build job against the cluster server.
2024-03-31 02:25:53,327 INFO:   Custom worker image build is disabled from server.
2024-03-31 02:25:53,334 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-03-31 02:25:53,695 INFO:   Initiating a new compile wsjob against the cluster server.
2024-03-31 02:25:53,822 INFO:   compile job id: wsjob-sm5xxuffwykga4ff7qzu8h, remote log path: /n1/wsjob/workdir/job-operator/wsjob-sm5xxuffwykga4ff7qzu8h
2024-03-31 02:26:03,872 INFO:   Poll ingress status: Waiting for job service readiness.
2024-03-31 02:26:33,870 INFO:   Poll ingress status: Waiting for job ingress readiness.
2024-03-31 02:26:53,894 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-03-31 02:26:57,900 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_9465229803081323743
2024-03-31 02:26:57,905 INFO:   Heartbeat thread stopped for wsjob-sm5xxuffwykga4ff7qzu8h.
2024-03-31 02:26:57,907 INFO:   Compile was successful!
2024-03-31 02:26:57,912 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.
2024-03-31 02:27:00,136 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-03-31 02:27:00,520 INFO:   Initiating a new execute wsjob against the cluster server.
2024-03-31 02:27:00,666 INFO:   execute job id: wsjob-nggut7dv9mrvxnygeys6id, remote log path: /n1/wsjob/workdir/job-operator/wsjob-nggut7dv9mrvxnygeys6id
2024-03-31 02:27:10,717 INFO:   Poll ingress status: Waiting for job running, current job status: Queueing, msg: job is queueing. Job queue status: current job is top of queue but likely blocked by running jobs, 1 execute job(s) running using 1 system(s), 1 compile job(s) running using 67Gi memory. For more information, please run 'csctl get jobs'.
2024-03-31 02:27:20,683 INFO:   Poll ingress status: Waiting for job running, current job status: Scheduled, msg: job is scheduled. 
2024-03-31 02:27:30,702 INFO:   Poll ingress status: Waiting for job service readiness.
2024-03-31 02:27:50,743 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-03-31 02:27:50,914 INFO:   Preparing to execute using 1 CSX
2024-03-31 02:28:18,875 INFO:   About to send initial weights
2024-03-31 02:28:54,502 INFO:   Finished sending initial weights
2024-03-31 02:28:54,505 INFO:   Finalizing appliance staging for the run
2024-03-31 02:28:54,541 INFO:   Waiting for device programming to complete
2024-03-31 02:30:53,218 INFO:   Device programming is complete
2024-03-31 02:30:54,090 INFO:   Using network type: ROCE
2024-03-31 02:30:54,091 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...
2024-03-31 02:30:54,127 INFO:   Input workers have begun streaming input data
2024-03-31 02:31:10,971 INFO:   Appliance staging is complete
2024-03-31 02:31:10,976 INFO:   Beginning appliance run
2024-03-31 02:31:31,980 INFO:   | Train Device=CSX, Step=100, Loss=9.48438, Rate=4891.79 samples/sec, GlobalRate=4891.80 samples/sec
2024-03-31 02:31:52,953 INFO:   | Train Device=CSX, Step=200, Loss=8.35938, Rate=4886.32 samples/sec, GlobalRate=4887.23 samples/sec
2024-03-31 02:32:14,248 INFO:   | Train Device=CSX, Step=300, Loss=7.91406, Rate=4839.70 samples/sec, GlobalRate=4860.74 samples/sec
2024-03-31 02:32:35,399 INFO:   | Train Device=CSX, Step=400, Loss=7.54688, Rate=4840.61 samples/sec, GlobalRate=4855.84 samples/sec
2024-03-31 02:32:56,624 INFO:   | Train Device=CSX, Step=500, Loss=7.46875, Rate=4830.94 samples/sec, GlobalRate=4849.54 samples/sec
2024-03-31 02:33:18,090 INFO:   | Train Device=CSX, Step=600, Loss=7.39844, Rate=4794.65 samples/sec, GlobalRate=4836.18 samples/sec
2024-03-31 02:33:38,919 INFO:   | Train Device=CSX, Step=700, Loss=7.35156, Rate=4867.50 samples/sec, GlobalRate=4847.43 samples/sec
2024-03-31 02:34:00,066 INFO:   | Train Device=CSX, Step=800, Loss=7.25000, Rate=4852.45 samples/sec, GlobalRate=4846.80 samples/sec
2024-03-31 02:34:21,285 INFO:   | Train Device=CSX, Step=900, Loss=7.21094, Rate=4836.53 samples/sec, GlobalRate=4844.47 samples/sec
2024-03-31 02:34:42,606 INFO:   | Train Device=CSX, Step=1000, Loss=7.07812, Rate=4816.24 samples/sec, GlobalRate=4840.27 samples/sec
2024-03-31 02:34:42,607 INFO:   Saving checkpoint at step 1000
2024-03-31 02:35:20,123 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_1000.mdl
2024-03-31 02:36:02,864 INFO:   Heartbeat thread stopped for wsjob-nggut7dv9mrvxnygeys6id.
2024-03-31 02:36:02,879 INFO:   Training completed successfully!
2024-03-31 02:36:02,879 INFO:   Processed 1024000 sample(s) in 211.558624233 seconds.

2024-04-01 15:37:29,773 INFO:   Effective batch size is 512.
2024-04-01 15:37:29,798 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in "model_dir_bert_large_pytorch" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.
2024-04-01 15:37:29,799 INFO:   No checkpoints were found in "model_dir_bert_large_pytorch".
2024-04-01 15:37:29,799 INFO:   No checkpoint was provided. Using randomly initialized model parameters.
2024-04-01 15:37:31,088 INFO:   Saving checkpoint at step 0
2024-04-01 15:37:58,220 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_0.mdl
2024-04-01 15:38:13,738 INFO:   Compiling the model. This may take a few minutes.
2024-04-01 15:38:13,739 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-01 15:38:15,097 INFO:   Initiating a new image build job against the cluster server.
2024-04-01 15:38:15,187 INFO:   Custom worker image build is disabled from server.
2024-04-01 15:38:15,194 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-01 15:38:15,470 INFO:   Initiating a new compile wsjob against the cluster server.
2024-04-01 15:38:15,569 INFO:   compile job id: wsjob-gfv8imwbhfhgcsq6zml5qi, remote log path: /n1/wsjob/workdir/job-operator/wsjob-gfv8imwbhfhgcsq6zml5qi
2024-04-01 15:38:25,606 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-01 15:38:55,618 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-01 15:38:59,449 INFO:   Pre-optimization transforms...
2024-04-01 15:39:05,484 INFO:   Optimizing layouts and memory usage...
2024-04-01 15:39:05,557 INFO:   Gradient accumulation enabled
2024-04-01 15:39:05,558 WARNING:   Gradient accumulation will search for an optimal micro batch size based on internal performance models, which can lead to an increased compile time. Specify `micro_batch_size` option in the 'train_input/eval_input' section of your .yaml parameter file to set the gradient accumulation microbatch size, if an optimal microbatch size is known.

2024-04-01 15:39:05,562 INFO:   Gradient accumulation trying sub-batch size 8...
2024-04-01 15:39:10,718 INFO:   Exploring floorplans
2024-04-01 15:39:18,361 INFO:   Exploring data layouts
2024-04-01 15:39:31,184 INFO:   Optimizing memory usage
2024-04-01 15:40:19,172 INFO:   Gradient accumulation trying sub-batch size 64...
2024-04-01 15:40:24,659 INFO:   Exploring floorplans
2024-04-01 15:40:33,470 INFO:   Exploring data layouts
2024-04-01 15:40:52,335 INFO:   Optimizing memory usage
2024-04-01 15:41:22,438 INFO:   Gradient accumulation trying sub-batch size 32...
2024-04-01 15:41:27,447 INFO:   Exploring floorplans
2024-04-01 15:41:34,077 INFO:   Exploring data layouts
2024-04-01 15:41:47,926 INFO:   Optimizing memory usage
2024-04-01 15:42:18,945 INFO:   Gradient accumulation trying sub-batch size 128...
2024-04-01 15:42:24,620 INFO:   Exploring floorplans
2024-04-01 15:42:35,594 INFO:   Exploring data layouts
2024-04-01 15:42:54,762 INFO:   Optimizing memory usage
2024-04-01 15:43:23,952 INFO:   Gradient accumulation trying sub-batch size 256...
2024-04-01 15:43:29,431 INFO:   Exploring floorplans
2024-04-01 15:43:44,748 INFO:   Exploring data layouts
2024-04-01 15:44:08,382 INFO:   Optimizing memory usage
2024-04-01 15:44:57,092 INFO:   Exploring floorplans
2024-04-01 15:45:00,793 INFO:   Exploring data layouts
2024-04-01 15:45:31,004 INFO:   Optimizing memory usage
2024-04-01 15:46:04,890 INFO:   No benefit from gradient accumulation expected. Compile will proceed at original per-box batch size 512 with 6 lanes

2024-04-01 15:46:04,940 INFO:   Post-layout optimizations...
2024-04-01 15:46:18,677 INFO:   Allocating buffers...
2024-04-01 15:46:21,243 INFO:   Code generation...
2024-04-01 15:46:34,679 INFO:   Compiling image...
2024-04-01 15:46:34,685 INFO:   Compiling kernels
2024-04-01 15:49:59,146 INFO:   Compiling final image
2024-04-01 15:52:51,552 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_8939750200954608837
2024-04-01 15:52:51,601 INFO:   Heartbeat thread stopped for wsjob-gfv8imwbhfhgcsq6zml5qi.
2024-04-01 15:52:51,604 INFO:   Compile was successful!
2024-04-01 15:52:51,609 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.
2024-04-01 15:52:54,064 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-01 15:52:54,359 INFO:   Initiating a new execute wsjob against the cluster server.
2024-04-01 15:52:54,468 INFO:   execute job id: wsjob-kedjbqxh65vxvjy4smwotc, remote log path: /n1/wsjob/workdir/job-operator/wsjob-kedjbqxh65vxvjy4smwotc
2024-04-01 15:53:04,504 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-01 15:53:34,548 INFO:   Poll ingress status: Waiting for job ingress readiness.
2024-04-01 15:53:54,593 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-01 15:53:54,724 INFO:   Preparing to execute using 1 CSX
2024-04-01 15:54:24,131 INFO:   About to send initial weights
2024-04-01 15:54:58,382 INFO:   Finished sending initial weights
2024-04-01 15:54:58,385 INFO:   Finalizing appliance staging for the run
2024-04-01 15:54:58,405 INFO:   Waiting for device programming to complete
2024-04-01 15:57:22,627 INFO:   Device programming is complete
2024-04-01 15:57:23,560 INFO:   Using network type: ROCE
2024-04-01 15:57:23,561 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...
2024-04-01 15:57:23,585 INFO:   Input workers have begun streaming input data
2024-04-01 15:57:48,298 INFO:   Appliance staging is complete
2024-04-01 15:57:48,304 INFO:   Beginning appliance run
2024-04-01 15:58:05,702 INFO:   | Train Device=CSX, Step=100, Loss=9.39062, Rate=2955.14 samples/sec, GlobalRate=2955.14 samples/sec
2024-04-01 15:58:23,296 INFO:   | Train Device=CSX, Step=200, Loss=8.70312, Rate=2928.03 samples/sec, GlobalRate=2932.37 samples/sec
2024-04-01 15:58:40,949 INFO:   | Train Device=CSX, Step=300, Loss=7.79688, Rate=2911.45 samples/sec, GlobalRate=2921.64 samples/sec
2024-04-01 15:58:58,545 INFO:   | Train Device=CSX, Step=400, Loss=7.39062, Rate=2910.43 samples/sec, GlobalRate=2918.66 samples/sec
2024-04-01 15:59:16,179 INFO:   | Train Device=CSX, Step=500, Loss=7.80469, Rate=2906.29 samples/sec, GlobalRate=2915.62 samples/sec
2024-04-01 15:59:33,762 INFO:   | Train Device=CSX, Step=600, Loss=7.53125, Rate=2909.64 samples/sec, GlobalRate=2914.99 samples/sec
2024-04-01 15:59:51,438 INFO:   | Train Device=CSX, Step=700, Loss=7.35156, Rate=2901.83 samples/sec, GlobalRate=2912.35 samples/sec
2024-04-01 16:00:08,975 INFO:   | Train Device=CSX, Step=800, Loss=7.27344, Rate=2912.44 samples/sec, GlobalRate=2913.25 samples/sec
2024-04-01 16:00:26,622 INFO:   | Train Device=CSX, Step=900, Loss=7.35938, Rate=2905.78 samples/sec, GlobalRate=2911.92 samples/sec
2024-04-01 16:00:44,082 INFO:   | Train Device=CSX, Step=1000, Loss=7.12500, Rate=2921.77 samples/sec, GlobalRate=2913.96 samples/sec
2024-04-01 16:00:44,083 INFO:   Saving checkpoint at step 1000
2024-04-01 16:01:20,048 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_1000.mdl
2024-04-01 16:01:47,961 INFO:   Heartbeat thread stopped for wsjob-kedjbqxh65vxvjy4smwotc.
2024-04-01 16:01:47,967 INFO:   Training completed successfully!
2024-04-01 16:01:47,967 INFO:   Processed 512000 sample(s) in 175.70607372 seconds.

2024-04-01 19:34:57,363 INFO:   Effective batch size is 2048.
2024-04-01 19:34:57,388 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in "model_dir_bert_large_pytorch" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.
2024-04-01 19:34:57,389 INFO:   No checkpoints were found in "model_dir_bert_large_pytorch".
2024-04-01 19:34:57,389 INFO:   No checkpoint was provided. Using randomly initialized model parameters.
2024-04-01 19:34:58,645 INFO:   Saving checkpoint at step 0
2024-04-01 19:35:26,552 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_0.mdl
2024-04-01 19:35:42,212 INFO:   Compiling the model. This may take a few minutes.
2024-04-01 19:35:42,213 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-01 19:35:43,487 INFO:   Initiating a new image build job against the cluster server.
2024-04-01 19:35:43,588 INFO:   Custom worker image build is disabled from server.
2024-04-01 19:35:43,593 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-01 19:35:43,900 INFO:   Initiating a new compile wsjob against the cluster server.
2024-04-01 19:35:44,011 INFO:   compile job id: wsjob-atnm6ms3hr7pplrxufuu9x, remote log path: /n1/wsjob/workdir/job-operator/wsjob-atnm6ms3hr7pplrxufuu9x
2024-04-01 19:35:54,053 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-01 19:36:24,070 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-01 19:36:28,753 INFO:   Pre-optimization transforms...
2024-04-01 19:36:33,870 INFO:   Optimizing layouts and memory usage...
2024-04-01 19:36:33,968 INFO:   Gradient accumulation enabled
2024-04-01 19:36:33,969 WARNING:   Gradient accumulation will search for an optimal micro batch size based on internal performance models, which can lead to an increased compile time. Specify `micro_batch_size` option in the 'train_input/eval_input' section of your .yaml parameter file to set the gradient accumulation microbatch size, if an optimal microbatch size is known.

2024-04-01 19:36:33,972 INFO:   Gradient accumulation trying sub-batch size 8...
2024-04-01 19:36:38,972 INFO:   Exploring floorplans
2024-04-01 19:36:46,536 INFO:   Exploring data layouts
2024-04-01 19:36:58,905 INFO:   Optimizing memory usage
2024-04-01 19:37:45,945 INFO:   Gradient accumulation trying sub-batch size 256...
2024-04-01 19:37:51,566 INFO:   Exploring floorplans
2024-04-01 19:38:07,770 INFO:   Exploring data layouts
2024-04-01 19:38:31,946 INFO:   Optimizing memory usage
2024-04-01 19:39:08,290 INFO:   Gradient accumulation trying sub-batch size 32...
2024-04-01 19:39:13,642 INFO:   Exploring floorplans
2024-04-01 19:39:21,543 INFO:   Exploring data layouts
2024-04-01 19:39:37,329 INFO:   Optimizing memory usage
2024-04-01 19:40:09,391 INFO:   Gradient accumulation trying sub-batch size 512...
2024-04-01 19:40:16,315 INFO:   Exploring floorplans
2024-04-01 19:40:19,971 INFO:   Exploring data layouts
2024-04-01 19:40:56,962 INFO:   Optimizing memory usage
2024-04-01 19:41:31,724 INFO:   Gradient accumulation trying sub-batch size 128...
2024-04-01 19:41:37,209 INFO:   Exploring floorplans
2024-04-01 19:41:47,968 INFO:   Exploring data layouts
2024-04-01 19:42:08,057 INFO:   Optimizing memory usage
2024-04-01 19:42:34,958 INFO:   Gradient accumulation trying sub-batch size 1024...
2024-04-01 19:42:40,137 INFO:   Exploring floorplans
2024-04-01 19:42:42,393 INFO:   Exploring data layouts
2024-04-01 19:43:15,019 INFO:   Optimizing memory usage
2024-04-01 19:43:43,933 INFO:   Exploring floorplans
2024-04-01 19:43:45,675 INFO:   Exploring data layouts
2024-04-01 19:44:23,962 INFO:   Optimizing memory usage
2024-04-01 19:45:16,246 INFO:   No benefit from gradient accumulation expected. Compile will proceed at original per-box batch size 2048 with 11 lanes

2024-04-01 19:45:16,286 INFO:   Post-layout optimizations...
2024-04-01 19:45:24,971 INFO:   Allocating buffers...
2024-04-01 19:45:28,415 INFO:   Code generation...
2024-04-01 19:45:44,545 INFO:   Compiling image...
2024-04-01 19:45:44,551 INFO:   Compiling kernels
2024-04-01 19:47:39,735 INFO:   Compiling final image
2024-04-01 19:50:09,846 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_12842439843636108263
2024-04-01 19:50:09,903 INFO:   Heartbeat thread stopped for wsjob-atnm6ms3hr7pplrxufuu9x.
2024-04-01 19:50:09,905 INFO:   Compile was successful!
2024-04-01 19:50:09,911 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.
2024-04-01 19:50:12,313 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-01 19:50:12,644 INFO:   Initiating a new execute wsjob against the cluster server.
2024-04-01 19:50:12,768 INFO:   execute job id: wsjob-s9anfw8cjb2rkcjuprggdg, remote log path: /n1/wsjob/workdir/job-operator/wsjob-s9anfw8cjb2rkcjuprggdg
2024-04-01 19:50:22,809 INFO:   Poll ingress status: Waiting for job running, current job status: Scheduled, msg: job is scheduled. 
2024-04-01 19:50:32,813 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-01 19:50:52,857 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-01 19:50:53,003 INFO:   Preparing to execute using 1 CSX
2024-04-01 19:51:23,244 INFO:   About to send initial weights
2024-04-01 19:51:57,550 INFO:   Finished sending initial weights
2024-04-01 19:51:57,553 INFO:   Finalizing appliance staging for the run
2024-04-01 19:51:57,604 INFO:   Waiting for device programming to complete
2024-04-01 19:54:06,992 INFO:   Device programming is complete
2024-04-01 19:54:08,013 INFO:   Using network type: ROCE
2024-04-01 19:54:08,014 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...
2024-04-01 19:54:08,065 INFO:   Input workers have begun streaming input data
2024-04-01 19:54:24,920 INFO:   Appliance staging is complete
2024-04-01 19:54:24,925 INFO:   Beginning appliance run
2024-04-01 19:54:54,939 INFO:   | Train Device=CSX, Step=100, Loss=9.48438, Rate=6848.74 samples/sec, GlobalRate=6848.74 samples/sec
2024-04-01 19:55:25,287 INFO:   | Train Device=CSX, Step=200, Loss=8.48438, Rate=6788.49 samples/sec, GlobalRate=6798.17 samples/sec
2024-04-01 19:55:55,819 INFO:   | Train Device=CSX, Step=300, Loss=7.77344, Rate=6740.00 samples/sec, GlobalRate=6767.73 samples/sec
2024-04-01 19:56:26,377 INFO:   | Train Device=CSX, Step=400, Loss=7.64062, Rate=6717.20 samples/sec, GlobalRate=6751.18 samples/sec
2024-04-01 19:56:56,860 INFO:   | Train Device=CSX, Step=500, Loss=7.37500, Rate=6718.00 samples/sec, GlobalRate=6744.62 samples/sec
2024-04-01 19:57:27,202 INFO:   | Train Device=CSX, Step=600, Loss=7.42188, Rate=6737.05 samples/sec, GlobalRate=6745.48 samples/sec
2024-04-01 19:57:57,700 INFO:   | Train Device=CSX, Step=700, Loss=7.25000, Rate=6723.93 samples/sec, GlobalRate=6741.13 samples/sec
2024-04-01 19:58:28,204 INFO:   | Train Device=CSX, Step=800, Loss=7.12500, Rate=6717.87 samples/sec, GlobalRate=6737.71 samples/sec
2024-04-01 19:58:58,962 INFO:   | Train Device=CSX, Step=900, Loss=7.25000, Rate=6682.20 samples/sec, GlobalRate=6728.80 samples/sec
2024-04-01 19:59:29,628 INFO:   | Train Device=CSX, Step=1000, Loss=7.14844, Rate=6679.99 samples/sec, GlobalRate=6723.74 samples/sec
2024-04-01 19:59:29,628 INFO:   Saving checkpoint at step 1000
2024-04-01 20:00:05,873 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_1000.mdl
2024-04-01 20:01:00,158 INFO:   Heartbeat thread stopped for wsjob-s9anfw8cjb2rkcjuprggdg.
2024-04-01 20:01:00,165 INFO:   Training completed successfully!
2024-04-01 20:01:00,165 INFO:   Processed 2048000 sample(s) in 304.592295123 seconds.

A batch size of 512 took less time but resulted in a larger loss. A batch size of 2048 took less time and resulted in a larger loss. 