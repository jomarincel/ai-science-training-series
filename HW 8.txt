LLMs can be used directly in science for generating DNA sequences or predicting protein folding, but they could also be used in science education. One might train a LLM on popular science articles and then deploy it as a chatbot on for instance a national lab's website. Then the same evaluation would apply to this model as to other chatbots: does the model produce accurate results, or does it hallucinate? Does it get the facts right? How well can it explain current research, its motivation and relationship to the corpus of scientific knowledge? Is it coherent? By using internal articles from the national lab in the backend, this LLM could outperform general purpose chatbots. Another evaluation metric might then be: how well does it fill in the gaps when presenting this information? 